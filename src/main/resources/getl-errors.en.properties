# Base
object.non_features=Not support feature
object.not_support=Not support{ %type%} "{feature}"{ (%detail%)}
object.internal_error=Internal error "{error}"{ (%detail%)}
object.non_instance=Object{ "%name%"} is not compatible with the {className} class
object.non_property=Non set property {prop}

# Parameters
params.required=Required to fill parameter "{param}"{ (%detail)%}
params.empty=Required to fill parameter "{param}"{ (%detail%)}
params.great_zero="{param}" must be greater than zero{ (current value %value%)}
params.great_equal_zero="{param}" must be equal or greater than zero{ (current value %value%)}

# Options
options.non_saved=No saved options for "{className}" object available

# Config
config.section_not_found=Section "{section}" not found

# Strings
strings.unknown_var=Unknown variable "{var}"{, available: %known_vars%}
strings.invalid_var_value=Variable "{var}" contains NULL
strings.invalid_var_type_map=Variable "{var}" is of type Map
string.invalid_object_name=Invalid object name "{name}"
string.invalid_password_length=Password must be between 16 and 32 characters
string.invalid_decode_args=The number of parameters in "{param}" must be at least three and odd

# Converts
convert.number.overflow=The value of the number {value} is out of range for allowed values from {min_value} to {max_value}

# Maps
map.unknown_sections=Unknown keys of sections {sections} are specified{ in method %method%}
map.unknown_keys=Unknown names found{ %keys%}
map.unknown_xsd_primitive_type=Unknown primitive XSD type "{type}"
map.invalid_section_type=Section "{section}" has type "{className}" other than "Map" type{ in method %method%}
map.invalid_arguments_type=Invalid arguments type for processing
map.invalid_list_item_format=Invalid format mask for list item
map.invalid_xsd_section=Invalid XSD include section{ %section%}
map.invalid_xsd_complex_type=Invalid or not found the complex type "{type}"
map.invalid_xsd_descr_type=Unknown XSD description for type "{type}"
map.invalid_xsd_union_oper=Invalid union operator for type "{type}"
map.invalid_xsd_simple_type=Invalid simple type "{type}"
map.fail_parse_node=Error parse attributes for xml node{ %node%}
map.non_xsd_resource_file=Xsd resource file "{path}" not found
map.non_xsd_type=Type "{type}" not found
map.required_xsd_field_type=Required type for field "{field}"
map.required_xsd_base_name=Required base name for type "{type}"

# File system
io.path.invalid=Invalid path "{path}"
io.dir.not_found=Directory "{path}" not found{ in "%search%"}
io.dir.already=Directory "{path}" already exists
io.dir.fail_create=Failed creating directory "{path}"
io.dir.fail_delete=Failed deleting directory "{path}"
io.file.not_found={%type% }File "{path}" not found{ in "%search%"}
io.file.empty=File "{path}" is empty
io.file.fail_delete=Failed deleting file "{path}"{ (%detail%)}
io.file.fail_delete_temp=Failed remove temporary file "{path}"
io.file.fail_rename=Failed renaming file "{path}"{ to "%dir%"}{ (%detail%)}
io.file.fail_swap_temp=Failed rename temporary file "{tempPath}" to "{path}"
io.file.fail_create=Failed creating new file "{path}"
io.file.already_exists=File "{path}" already exists

# Logs
logs.non_filename=Log filename not set

# Logins
logins.user_not_found=User "{user}" not found in the list of logins
logins.no_users=There are no saved logins to switch to

# Datasets
dataset.non_fields=Not set list of fields
dataset.non_key_fields=Not set list of key fields
dataset.non_connection=Connection is not specified
dataset.fields_incorrect_count_params=The number of fields to change "{countFieldDataset}" does not match the number of parameters passed "{countFieldParam}"
dataset.fields_not_detected=Fields not detected{ in file "%file%"}
dataset.field_not_found=Field "{field}" not found{ (%detail%)}
dataset.field_type_not_compatible=Field "{field}" with type "{source_type}" is not compatible from type "{dest_type}"
dataset.field_length_not_compatible=Field "{field}" must have a length of at least "{length}{ and must have a precision of at least "%precision%}"
dataset.field_null_not_compatible=Field "{field}" cannot be nullable
dataset.field_key_not_compatible=Field "{field}" must be a key field
dataset.field_default_not_compatible=Field "{field}" has a default expression that does not match
dataset.unnecessary_field=There is an unnecessary field "{field}" in dataset
dataset.field_key_value_required=Required value for key field {field}{ in row %row%}
dataset.field_type_required="{field}" field must be of type "{type}"
dataset.field_length_required=Requires "{field}" field length{ with "%type%" type}
dataset.field_prec_required=Requires "{field}" field precision{ with "%type%" type}
dataset.deny_fields_change=Does not allow changing the field set
dataset.invalid_connection=Connection to "{className}" class is only allowed
dataset.invalid_type_update=Invalid update field type "{type}"
dataset.invalid_status=Not available for "{operation}" operation (current status is "{status}")
dataset.invalid_field_name=Field name cannot be empty{ (%detail%)}
dataset.invalid_field_type=Not support field type {type}{ [%typeName%]}{ for field %field%}
dataset.invalid_lookup_strategy=Invalid strategy value "{strategy}" for method lookup
dataset.invalid_check_structure=Incorrect fields: {errors}
dataset.fail_read_fields=Fail reading fields{ for %objectName%}
dataset.fail_create=Fail creating dataset
dataset.fail_drop=Fail dropping dataset
dataset.fail_bulk_load=Fail bulk loading files
dataset.fail_open_write=Fail opening dataset for write
dataset.fail_write=Fail writing row to dataset
dataset.fail_close_write=Fail closing dataset after write
dataset.fail_read_schema_file=Fail reading schema file{ "%file%"}
dataset.fail_move_field_to_last=Failed to move field "{field}" to the end of the list
dataset.fail_read_columns=Error reading columns of table {table}
dataset.fail_read_primary_key_constraint=Error reading primary key constraint

# Connections
connection.not_connected=Not connected
connections.non_tran=Not started transaction
connection.invalid_constructor=Class "{className}" has not corrected constructor
connection.invalid_class=Invalid "{className}" instance class
connection.invalid_codepage=Invalid code page "{code_page}"
connection.illegal_codepage=If no support for code page "{code_page}" is available in this instance of the Java virtual machine
connection.already_connect=Already connected
connection.already_disconnect=Already disconnected
connection.fail_disconnect=Server disconnect error
connection.fail_start_tran=Fail starting transaction
connection.fail_commit_tran=Fail committing transaction
connection.fail_connect=Fail connecting to server{, attempt number %attempt%}

# ClickHouse
clickhouse.invalid_ssl_mode=Invalid ssl mode "{mode}", allow NONE or STRICT

# CSV
csv.empty_header=Detected empty field name in header: {header}
csv.split_incorrect=When working with local data, the "isSplit" option must be turned off
csv.invalid_preset=Preset "{preset}" not defined
csv.need_standard_row_delimiter=Non standard row delimiter
csv.invalid_blob=Invalid blob value in file
csv.read_error=Error in line {line}, column {colNumber} [{colName}]{, %error%}
csv.write_map_null=Null map was passed for writing
csv.write_map_empty=Empty map was passed for writing
csv.invalid_row_delimiter=Non-standard line separator can only be one character

# Structure files
struct_files.invalid_config_attrs=Error load config, where name is not setting in section attributes {attr}

# Web services
web_files.invalid_request=Invalid request method "{request}" (allowed GET or POST)
web_files.content_not_found=No data in response: {response}

# Data files
dataset.non_filename=Not set file name
dataset.non_path=Not set path
dataset.deny_gzip_append=Appending to gzip files is not supported

# Excel
excel.cell_type_not_compatible=Cell type "{cellType}" not compatible with "{type}"
excel.invalid_sheet_list=Workbook Excel sheet list "{list}" not found
excel.invalid_sheet_num=Workbook Excel sheet number "{number}" not found
excel.invalid_extension="Invalid {extension}" extension for file "{path}" (use xls or xlsx)
excel.non_sheet=Not found sheet in workbook Excel

# File managers
fileman.not_connect=Not connected
fileman.already_connect=Already connected
fileman.already_disconnect=Already disconnected
fileman.fail_disconnect=Server disconnect error
fileman.fail_connect=Server connect error
fileman.invalid_object=Not support "{path}" object type
fileman.fail_change_dir_up_root=Can not change directory to up with root directory "{path}"
fileman.non_sort=No sorting set for limit files
fileman.same_path_and_mask=Not allowed to specify "path" and "maskFile" at the same time
fileman.invalid_sort_field=Sort contains an unknown field "{field}"
fileman.fail_delete_double=Internal error on delete double files name for build list files
fileman.need_read_file_list=File list not read

# FTP manager
fileman.ftp.invalid_list_type=Invalid list type object "{type}"

# Path
filepath.invalid_name="{name}" keyword cannot be a variable name

# JDBC
jdbc.need_table=Requires compatibility with type table{ (%detail%)}
jdbc.need_jdbc_dataset=Requires compatibility with type JDBC dataset{ (%detail%)}
jdbc.key_field_not_found=Primary column "{field}" not found
jdbc.invalid_commit=Cannot use commit in auto-commit mode
jdbc.invalid_rollback=Cannot use rollback in auto-commit mode
jdbc.invalid_field_expression=Not support expression for field "{field}"{ (%detail%)}
jdbc.invalid_write_oper=Invalid write operation "{operation}"
jdbc.fail_query_build=Failed to build sql for query
jdbc.different_connections=Different connections with "{source}" source
jdbc.connection.fail_connect=Fail creating driver "{driver}" for url "{url}"
jdbc.connection.deny_change_property=Not allow change session property value
jdbc.connection.fail_change_prop=Error change session property "{name}" to value "{value}"
jdbc.connection.invalid_dataset_type=Invalid dataset type "{type}"
jdbc.connection.invalid_isolation_level=Invalid isolation level "{level}"
jdbc.connection.only=JDBC connection only allowed
jdbc.table.not_found=Table {table} does not exist or does not have access rights to it
jdbc.table.non_table_name=Not set table name or does not have access rights to table
jdbc.table.only=JDBC table only allowed
jdbc.table.already_exists=Table already exists in database
jdbc.table.fail_read_fields=Fail reading fields, table {table} does not exist or does not have access rights to it
jdbc.table.schema_fields_not_found=Fields description not found for schema file "{path}"
jdbc.table.bulkload_files_not_set=Required to specify the names of the uploaded files in "files"
jdbc.table.bulkload_invalid_files=For option "files" you can specify a string type, a list of strings or a Path object
jdbc.table.bulkload_remote_load_file_copy=Copy file is not supported for remote load
jdbc.table.bulkload_remote_load_file_delete=Remove file is not supported for remote load
jdbc.table.bulkload_remote_code=Prepare files code not supported for remote load
jdbc.table.bulkload_remote_package_need=Required load package mode for remote load
jdbc.table.bulkload_remote_files_list_deny=List of files in files not allowed for remote load files
jdbc.table.bulkload_remote_story_deny=Story table is not supported for remote file bulk load on cluster nodes
jdbc.table.index.invalid_columns=Invalid type of property "columns" in "{index}" index
jdbc.table.synch.already_temp_table=Table "{table}" for connection "{connection}" in the database has an existing buffer table {buffer_table}, need to delete it or rename it to the destination table if it does not exist
jdbc.query.non_script=No query specified

# History points
historypoints.invalid_save_method=Invalid save method "{method}" (allowed INSERT or MERGE)
historypoints.invalid_source_type=Invalid source type "{type}" (allowed IDENTITY or TIMESTAMP)
historypoints.table_many_rows=More than one entry found in value storage table {table}

# Sql scripter
sqlscripter.invalid_syntax=Incorrect "{operator}" operator syntax in [{sql}]
sqlscripter.save_point_non_variable=Variable {variable} must contain the value in "SAVE_POINT" operator [{sql}]
sqlscripter.run_file_non_filename=No file name is specified in the "RUN_FILE" operator
sqlscripter.switch_login_need=Required login from "SWITCH_LOGIN" operator
sqlscripter.for_non_query=No query specified in "FOR" operator [{sql}]
sqlscripter.for_non_do=Keyword "DO" not found in "FOR" operator [{sql}]
sqlscripter.for_non_body=No script specified in "FOR" operator [{sql}]
sqlscripter.if_non_query=No query specified in "IF" operator [{sql}]
sqlscripter.if_non_do=Keyword "DO" not found in "IF" operator [{sql}]
sqlscripter.if_non_body=No script specified in "IF" operator [{sql}]
sqlscripter.command_non_body=No script specified in "COMMAND" operator [{sql}]
sqlscripter.invalid_exit_value=Incorrect value "{value}" for returning result of EXIT command

# Kafka
kafka.topic_already=Topic {topic} already exists
kafka.topic_not_found=Topic {topic} not found
kafka.invalid_offs_for_register=Invalid value "{value}" for "offsetForRegister" read option with Kafka dataset

# Vertica
vertica.temporary_view_privileges_not_support=Not support "{param}" parameter from temporary view
vertica.invalid_escape_char=Escaping can only be one character for bulk load
vertica.invalid_field_delimiter=Field delimiter can only be one character for bulk load
vertica.invalid_row_delimiter=Row delimiter can only be one character for bulk load
vertica.invalid_quote=Qute can only be one character for bulk load
vertica.invalid_table_schema_privileges=Invalid schema table privileges "{value}", allowed "INCLUDE" or "EXCLUDE"
vertica.required_parser_func_name=Required parser function name
vertica.invalid_field_separator_value=Field separator with unicode value 1 is not supported
vertica.invalid_quote_value=Quote separator with unicode value 1 is not supported
vertica.invalid_row_separator_value=Row separator with unicode value 1 is not supported
vertica.invalid_segmentation=Invalid segmented options
vertica.not_support_null_value_when_bulkload_not_escaped=Vertica driver not support "nullAsValue" option, when bulk loading not escaped file
vertica.invalid_bulkload_parameter_files=Parameter "{param}" must be of type List

# Utils
utils.web.invalid_authentication_type=Invalid authentication type "{type}" (allowed BASIC or NTLM)
utils.web.non_login=Authentication "{type}" requires login
utils.web.invalid_status=Returned status {status} when reading data from url "{url}"{ for service "%service%}, response: {response}
utils.web.allowed_only_get_method=Only the GET method is supported
utils.web.invalid_login=Login without authentication type specified
utils.web.invalid_password=Password without login

# Flow
flow.copy.incorrect_saveSourceFieldsInErrorsDataset=Param "{param}" is not allowed if error logging of source fields is enabled

# Getl Dsl
dsl.owner_required=Not set Getl instance{ %detail%}
dsl.non_main_operator=Required argument "runclass" or "workflow" or "workflowfile" for running
dsl.env_vars_not_found=The following OS environment variables required to run were not found: {variables}
dsl.duplicate_service=Duplicate init service code
dsl.need_abort_work=A flag was found to stop the process{ "%process%"}
dsl.object.not_found={%type% }"{repname}" not found in repository{ %repository%}
dsl.object.not_register=Object not registered in repository
dsl.object.non_unit_test_mode=This object working is allowed only with the test mode enabled
dsl.object.need_group_name=Not set group name in "{repname}"
dsl.object.already_register_by_name=Object "{repname}" is already registered with type "{type}" (class "{className}")
dsl.object.already_register_by_class=Object "{repname}" is registered in the repository as another class "{className}" with type "{type}"
dsl.object.already_register_by_file=File "{file}" cannot be loaded to registered object "{repname}" (class "{className}") in the repository
dsl.repository.not_found=Repository "{repository}" not found
dsl.repository.already_register=Repository "{repository}" is already registered
dsl.repository.non_path=File storage path for repository objects not set in property "storagePath"
dsl.repository.deny_path_resource=The repository path cannot be resource path
dsl.repository.invalid_method_annotation=Method "{method}" has no object type specified in annotation "@SaveToRepository"
dsl.repository.invalid_annotation_object_type=Unknown type "{type}" in annotation "@SaveToRepository" of method "{method}"
dsl.repository.invalid_story_dataset=It is allowed to use JDBC tables or CSV files to store the history of saving objects
dsl.repository.invalid_object_file=Description of repository object in file "{file}" was not found
dsl.repository.invalid_file_env=Environment "{fileEnv}" from file "{file}" does not match expected environment "{env}"
dsl.repository.invalid_config_file=Invalid configuration file name "{file}" from repository "{repository}"
dsl.repository.invalid_list_lazy_option=The "loadLazyObjects" parameter cannot be false if filtering is specified for the list method
dsl.repository.unknown_other_type=Unknown other type "{type}" in annotation "@SaveToRepository" of method "{method}"
dsl.repository.need_auto_loading=Lazy loading requires "autoLoadFromStorage" setting to be enabled
dsl.repository.need_env=Required to specify "env" parameter in annotation "@SaveToRepository" for method "{method}"
dsl.repository.fail_save_object=Error saving object "{repname}" from repository "{repository}" to file "{file}"
dsl.repository.fail_register_object=Fail register{ %type%} object{ %repname%} in repository{ %detail%}
dsl.repository.fail_load_object_from_file=File "{file}" was not found to load object "{repname}" to the repository
dsl.repository.fail_load_object_from_resource=Resource file "{file}" was not found to load object "{repname}" to the repository "{className}"
dsl.repository.fail_reload_object=Object "{repname}" is already registered in repository "{className}" and cannot be reloaded from file "file"
dsl.repository.fail_delete_resource=Not allowed to delete storage directory of repository "{className}" in resources
dsl.repository.fail_rename_resource=Renaming object "{repname}" is not supported for resource repository "{className}"
dsl.repository.fail_rename_exists=Name "{repname}" is registered in repository "{className}" and is not available for renaming from object "{origName}"
dsl.invalid_object_class=Class "{className}" is not supported for object type "{type}"
dsl.invalid_control_dataset=To control the operation of processes, a dataset with types "table" or "csv" can be used, but use "{className}" dataset
dsl.invalid_connection=Invalid connection class "{className}"
dsl.invalid_jdbc_connection=Invalid JDBC connection class "{className}"
dsl.invalid_dataset=Invalid dataset type "{className}"
dsl.invalid_jdbc_dataset=Invalid JDBC dataset type "{className}"
dsl.invalid_file_dataset=Invalid file dataset type "{className}"
dsl.invalid_other_dataset=Invalid services dataset type "{className}"
dsl.invalid_class=Class "{className}" is not "Getl" class
dsl.invalid_script=Class "{className}" is not "Script" class
dsl.invalid_instance_args=Type "{className}" is not supported as method parameter
dsl.invalid_job_start_parameter=Parameter "{param}" can only be used for workflow
dsl.invalid_main_operator=Simultaneous specification of "runClass" and "workflow" parameters is not allowed
dsl.invalid_story_path="{path}" directory for "savingStoryDataset" not found
dsl.invalid_object_name_format=Incorrect syntax of repository object name "{repname}" (allow "group_name[.sub_group_name[.sub_group_name]]:object_name")
dsl.invalid_group_name_without=Group name {"group"} should not contain the object name
dsl.invalid_group_name_temp_deny=Group name "{group}" cannot start with character '#' for object name "{repname}"
dsl.invalid_group_name_chars=Incorrect characters in group name "{group}" for object name "{repname}"
dsl.invalid_object_name_chars=Incorrect characters in group name "{objname}" for object name "{repname}"
dsl.invalid_script_property=Script does not have a public field "{prop}"
dsl.invalid_date_format=Incomprehensible date string "{value}"
dsl.invalid_convert_num_to_date=Not allowed to specify the date as the number "{value}"
dsl.invalid_time_format=Incomprehensible time string "{value}"
dsl.invalid_convert_num_to_time=Not allowed to specify the time as the number "{value}"
dsl.invalid_convert_path=Can't convert "{value}" to file path
dsl.invalid_instance_object="{name}" is not {type}
dsl.invalid_code_owner=Code owner expected "{type}"
dsl.deny_dsl_when_init=Cannot be called dsl code during Getl object initialization
dsl.deny_unittest_on_prod=Not allowed to enable unit testing mode for prod environment
dsl.deny_filter_group_threads=Using group filtering within a threads is not allowed
dsl.deny_group_for_temp_object=Cannot specify group name "{group}" for temporary object "{repname}"
dsl.deny_temp_object_for_group=Сannot specify object name "{objname}" starting with '#' when a group name "{group}" is specified
dsl.deny_threads_filter=Using group "{group}" filtering within a threads is not allowed
dsl.deny_threads_register=Registering object "{repname}" as type "{type}" within a threads is not allowed
dsl.etl.fail_update_row=Row change error in "etl.addRow" operator for table {table}, changed {count} rows, expected 1 row
dsl.etl.fail_delete_row=Row delete error in "etl.deleteRow" operator for table {table}, deleted {count} rows, expected 1 row
dsl.etl.fail_find_row=Row find error in "etl.findRow" operator for table {table}, read {count} rows, expected 1 row
dsl.etl.fail_find_row_field_type=The "{field}" field with the "{type}" type of the {table} table is not readable by the "etl.findRow" operator

# Models
dsl.model.non_source_connection=Does not specify a source connection
dsl.model.non_dest_connection=Does not specify a destination connection
dsl.model.non_dest_table=Destination table not specified for table {table}"
dsl.model.dest_table_not_found=For table "{table}", a non-existing table of destination "{destTable}" is specified
dsl.model.super_class_not_found=Can't find super class
dsl.model.dataset_not_found=Dataset "{dataset}" that is not found in the repository
dsl.model.table_not_found=Table "{table}" not found in the model
dsl.model.connection_not_registered=Connection "{connection}" that is not registered in the repository
dsl.model.dataset_not_registered=Dataset "{dataset}" that is not registered in the repository
dsl.model.invalid_destination_field=For source table "{table}" in the mapping, a non-existent field "{field}" is specified in the destination table {destTable}
dsl.model.invalid_used_object_class=Invalid used object type "{class}"
dsl.model.invalid_attribute_type=Error converting the value "{value}" of the model attribute "{attr}" into a {type}
dsl.model.invalid_connection_for_dataset=The connection of dataset "{dataset}" does not match the specified connection
dsl.model.invalid_attributes=Detect unknown model attributes [unknown_attrs] (allowed [attrs])
dsl.model.story_dataset_not_support_clear=Story dataset "{dataset}" not support clear

dsl.model.datasets.non_increment_dataset=Table "{table}" has an incremental field, but the model does not have an incremental dataset
dsl.model.datasets.non_parent_dataset=Required parent dataset for table "{table}"
dsl.model.datasets.invalid_increment_field_type=Field "{field}" of table "{table}" has type "{type}", which is not compatible with incremental manager (allowed types INTEGER, BIGINT, NUMERIC, DATE, DATETIME, TIMESTAMP WITH TIMEZONE types)
dsl.model.datasets.invalid_child_dataset=Table "{table}" does not support working with local data and cannot be used as a child dataset
dsl.model.datasets.increment_field_not_found=Increment field "{field}" was not found in dataset "{table}"
dsl.model.datasets.link_field_not_found=Link field "{field}" in table "{table}" not found in parent table "{parent_table}"
dsl.model.datasets.required_parent_table=The "{table}" table has the "{field}" link field set without specifying the parent table

dsl.model.monitor_rules.non_param=Requires "{prop}" value for "{rule}" rule
dsl.model.monitor_rules.invalid_rule_query_duplicate=Duplicate "{codes}" codes are returned for rule "rule"
dsl.model.monitor_rules.invalid_status_table_duplicate=More than one "{code}" code row found for rule "{rule}" in status table {table}
dsl.model.monitor_rules.invalid_rule=Unknown rule "{rule}" in {table} status table

dsl.model.reference_files.dest_path_not_found=Destination path "{path}" not found from "{file}" reference file
dsl.model.reference_vertica_tables.invalid_schema=The schema "{schema}" of table {table} cannot be a model schema
dsl.model.reference_vertica_tables.invalid_user=Could not find current user "{user}" in Vertica connection "{connection}"
dsl.model.reference_vertica_tables.invalid_copy_type=Unknown copy type "{type}" (allowed ETL, BULKLOAD and EXPORT)
dsl.model.reference_vertica_tables.invalid_null_value=Option "nullAsValue" is only supported when using copy type "BULKLOAD"
dsl.model.reference_vertica_tables.fail_fill={source_rows} rows were read from the reference table {source_table}, but {dest_rows} rows were written to work table {dest_table}

dsl.model.workflows.unknown_script=Script "{script}" not defined{ (%detail%)}
dsl.model.workflows.unknown_step=Step "{step}" not defined{ (%detail%)}
dsl.model.workflows.invalid_start_step=It is allowed to specify no more than one "start" step in the workflow
dsl.model.workflows.invalid_class=For script "{script}" of step "{step}", a non-existent class "{className}" is specified or the required code library is not attachment
dsl.model.workflows.invalid_save_mode_script=Script "{script}" by class "{className}" in step "{step}" does not support repository object saving mode and cannot be used when this mode is enabled
dsl.model.workflows.invalid_run_class=Class "{className}" for step "{step}" is not compatible with Getl scripts
dsl.model.workflows.invalid_script_param_type=Cannot convert value with type "{param_type}" to type "{script_param_type}" of parameter "{param_name}" for script "{script}" of step "{step}"
dsl.model.workflows.step_already=Step "{step}" is already defined in the workflow
dsl.model.workflows.invalid_step_name=Step name "{step}" contains invalid characters
dsl.model.workflows.deny_threads_for_save=Number of threads in step "{step}" must be equal to 1 when using the script for saving repository objects

dsl.model.map_tables.invalid_dest_some=Cannot use the same dataset {table} for source and destination
