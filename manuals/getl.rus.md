# Кратко про Getl
Groovy ETL (Getl) - open source проект на Groovy, разрабатываемый с 2012 года для автоматизации загрузки и обработки 
данных с разных источников. 

Getl является заменой классических ETL для проектов, где требуется быстро и много загружать данные из разнообразных 
файловых источников со сложными форматами и таблиц РСУБД. В Getl не нужно разрабатывать для каждой таблицы или формата файла
свой процесс. Достаточно на его простом и мощном ETL языке создать шаблоны с нужной логикой работы с данными и источниками
под логику программной архитектуры вашего проекта и вызывать их, передавая с какими источниками работать. Getl является
интеллектуальным ETL инструментом и самостоятельно создаст правильный маппинг между копируемыми данными, приведет к нужным
типам и выполнит задачу, скомпилировав ее на лету в байт код для выполнения на Java.

Для Micro Focus Vertica в Getl содержится расширенная функциональность, которая позволяет реализовывать сложные решения 
для захвата, доставки и расчета данных для этого хранилища данных. 
 
* Исходный код проекта располагается в GitHub проекте [getl](https://github.com/ascrus/getl)
* Русскоязычная документация располагается в [GitHub Pages](https://github.com/ascrus/getl)
* На базе Getl разработана библиотека шаблонов для облегчения разработки задач под Micro Focus Vertica, исходные коды
которой выложены на GitHub проекте [getl.vertica](https://github.com/ascrus/getl.vertica)

# Примеры работы
Примеры работы с его классами и Dsl языком можно найти в юнит тестах самого проекта Getl. 
Примеры работы с Getl на базе H2 Database можно посмотреть на GitHub проекте
[Getl examples](https://github.com/ascrus/getl-examples).

# Почему Groovy
Чтобы обеспечить баланс между простой разработки сценариев, гибкостью работы с плавающими структурами и скоростью работы
с данными, Getl был разработан на языке Groovy с использованием всех его достоинств: динамическая компиляция во время 
выполнения, статическая компиляция для ускорения выполнения нагруженных участков кода, поддержка разработки DSL расширений, 
управление видимостью объектов в коде замыканий, полная совместимость с кодом, написанным на Java, расширение языка 
для упрощения работы с массивами, JDBC источниками, временными типами и т.д. 
 
# Развитие
С 2012 года Getl научился работать с инкрементальным захватом данных, с группами файлов на разных файловых 
системах, стал поддерживать язык хранимых процедур для разработки скриптов под Vertica и обзавелся 
и собственным специализированным DSL языком для разработки ETL процессов и повторно используемых шаблонов.

# Какие задачи удобно решать с помощью Getl
Getl полезен, если вам требуется:
* Копировать массивы данных между разными JDBC совместимыми базами данных и файловыми источниками;
* Обеспечить инкрементальный захват данных и загрузку изменений в хранилища данных;
* Копировать, обрабатывать или удалять по заданным условиям файлы на файловых источниках;
* В пилотных проектах взять таблицы источника данных, развернуть совместимые структуры для хранилища данных и скопировать 
данные с источника в таблицы хранилища данных;
* Организовать запуск и отладку ETL процессов между средами разработки, тестирования и эксплуатации;
* Для облегчения тестирования создать модели эталонных данных и файлов и автоматизировать инициализацию данными для
запускаемых задач из unit-тестов;
* Для контроля состояния таблиц в хранилище данных описать правила времени из заполнения и автоматизировать процесс
мониторинга данных в таблицах;
* Разработать для автоматизации процессов собственные шаблоны для захвата, загрузки и обработки данных на простом ETL языке под
вашу используемую в проекте бизнес логику согласно вашей выбранной архитектуре хранилища данных.       

# Требования к тем, кто хочет работать на Getl
Для того, чтобы начать писать на Getl процессы ETL, сначала нужно на базовом уровне изучить два продукта:
1. Groovy, это можно сделать прямо на сайте "Хабрахабр" за 15 минут: [статья Groovy за 15 минут](http://habrahabr.ru/post/122127/).
1. Любой IDE, который поддерживает Java и Groovy и обеспечивает комфортную разработку и отладку кода, например 
[JetBrains IntelliJ IDEA](https://www.jetbrains.com/ru-ru/idea/documentation/) или 
[Eclipse](https://www.ibm.com/developerworks/ru/library/os-eclipse-platform/).
 
# Подключение Getl к проекту
Если пользуетесь _gradle_, то пропишите в build.gradle проекта ссылку на Getl:
```groovy
dependencies {
    compile(group: 'net.sourceforge.getl', name: 'getl', version:'4.5.*')
}
```
* Getl выкладывается в Maven Central, поэтому указывать репозиторий не нужно.
* Getl уже содержит ссылки на все зависимые библиотеки, поэтому в проект будут подтянуты Groovy и остальные библиотеки, 
требуемые для его работы.

Для работы с JDBC драйверами РСУБД в проекте рекомендуется сделать директорию _jdbc_ и скопировать туда
их jar файлы. Тогда в gradle можно указать их подключение в проект для запуска и отладки проекта в IDE:
```groovy
dependencies {
    compile(group: 'net.sourceforge.getl', name: 'getl', version:'4.5.*')
    compile fileTree(dir: 'jdbc')
}
```
P.S. Если проекту будет нужно одновременно использовать разные версии JDBC драйвера, в Getl возможно при описании соединения к РСУБД указать в свойстве "_driverPath_" 
путь к конкретному jar файлу, который потребуется использовать для этого соединения.
 
# Использование Getl в Groovy классах и скриптах
Для использования Getl в классах Groovy достаточно использовать статический метод Dsl класса Getl. Внутри кода Dsl доступны все функции Getl и можно разрабатывать любые сценарии:   
```groovy
package demo

import getl.lang.Getl
  
class GroovyApp {
    static void main(def args) {
      helloWorld()
    }

    void helloWorld() {
        Getl.Dsl {
            logInfo 'Привет мир!'
        }
    }
}
```
Вызов программы с командной строки будет такой же, как для любого Java приложения. Для этого нужно скомпилировать проект в jar файл с 
помощью Gradle и вызывать с указанием запускаемого класса:
```shell script
java -cp myproj.jar demo.GroovyApp
```
P.S. В class path (-cp) также требуется добавить пути к jar файлам Getl и используемых JDBC драйверов.

Классы удобно использовать там, где требуются возможности ООП (наследование, ОРМ, библиотеки статических функций и т. д.). Для ETL/ELT процессов это не требуется и усложняет разработку. 
Помимо классов, Groovy поддерживает разработку кода в виде сценариев (скриптов):
```groovy
// Файл GetlDemoScript.groovy
package demo

import groovy.transform.BaseScript
import getl.lang.Getl
import groovy.transform.Field

@BaseScript Getl main

@Field String name = 'приятель'

logInfo "Привет $name!"
``` 
1. Аннотация "_BaseScript_" указывает Groovy, что скрипт будет выполняться на Dsl языке Getl. Groovy автоматически при компиляции создаст Java класс на этот сценарий, 
который наследуется от Getl и реализует всю логику скрипта в методе "_run_" этого класса.
1. Переменная "_main_" при работе скрипта будет содержать ссылку на экземпляр объекта класса сценария. В случаях, когда локальные методы или переменные будут перекрывать именованием
методы и переменные Getl, переменная _main_ позволит все равно получить к ним доступ.
1. Переменная "_name_" с дескриптором "_@Field_" является параметром, который можно задать при вызове скрипта с командной строки или из другого места кода.

Сценарий можно вызвать даже без компиляции с помощью Groovy, если он установлен в ОС. Для этого нужно скопировать по пути инсталляции Groovy в директорию "_libs_" все jar файлы, 
нужные для работы Getl, включая JDBC драйвера. Рекомендуется вызывать скрипт из главной директории проекта, если в проекте используются пакеты:
```shell script
groovy demo/GetlDemoScript.groovy vars.name=друг
```

Для разработки сложных проектов рекомендуется использовать IDE с поддержкой Java и Groovy. В таком случае становится доступна полноценная разработка, отладка и сборка проекта в jar файл. 
Getl поддерживает хранение конфигурационных файлов и SQL скриптов в ресурсных файлах проекта, это облегчает сборку и сопровождение проектов. Для запуска сценариев из jar файла в классе 
Getl реализован метод "_main_" для запуска приложений, которому можно в параметре "_runclass_" передать полное имя скрипта Groovy из проекта для запуска:
```shell script
java -cp myproj.jar getl.lang.Getl runclass=demo.GetlDemoScript vars.name=друг
``` 
P.S. В class path (-cp) также требуется добавить пути к jar файлам Getl и используемых JDBC драйверов.

# Общий синтаксис сценариев Getl
Для сценариев Getl доступны следующие секции и операторы:
* Параметры скрипта @Field: можно задать при вызове скрипта из командной строки.
* Мета-методы скрипта:
	* init: инициализация скрипта до передачи параметров и выполнения
	* check: проверка скрипта после инициализации, установки параметров, но перед выполнением
	* error: пост-обработка возникшей ошибки в скрипте
	* done: финализация скрипта после завершения его работы
* configuration: управление конфигурацией работы приложения 
* logging: управление логом приложения
* операторы log*: запись сообщений в лог и консоль с требуемым уровнем уведомления
* операторы callScript и callScripts: многократный или однократный вызов другого скрипта
* ifRunAppMode и ifUnitTestMode: операторы ветвления кода для промышленной и тестовой эксплуатации
* profile: профилирование выполнения блоков кода
* thread: выполнение блока кода в множестве потоков для заданного списка элементов
* <тип>Connection: работа с соединениями источников
* <тип>Table, <тип>Dataset, view, query: работа с таблицами, файлами, представлениями и запросами источников
* fileman: чтение, запись и копирование между файловыми источниками
* etl: чтение, запись и копирование между источниками данных
* sql: выполнение пакетных скриптов SQL
* models: группировка объектов в модели эталонных данных и модели работы с данными
* testCase: выполнение проверок unit-тестов junit внутри кода скрипта

Пример полного сценария Getl:
```groovy
package пакет

import groovy.transform.BaseScript
import groovy.transform.Field
import getl.lang.Getl

// Определение скрипта как Getl и его переменной инстанса
@BaseScript Getl main

// Входные параметры сценария, которые можно задать при его вызове
@Field Date param1 = значение_по_умолчанию  

// Инициализация работы сценария
void init() {
    // Управление конфигурацией
    configuration { 
        // Загрузить файл конфигурации из ресурсных файлов проекта
        load 'resource:/конфигурация.conf'
    }
 
    // Управление файлом лога сценария
    logging {
        logFileName = 'путь/процесс.log'
    }
}

// Проверка параметров сценария до его выполнения
void check() {
	assert param1 != null, 'Надо указать параметр!'
	// Код дополнительной проверки, который не выполняется, если сценарий запущен из под unit-теста
	ifRunAppMode {
		assert param1 <= new Date(), 'Параметр не может быть больше текущего времени!'
	}
}

// Вызывается, если во время работы сценария произошла ошибка
void error(Exception e) {
  logError "Произошла ошибка: ${e.message}"
}

// Финализация после работы сценария
void done() {
  // Удалить временные ресурсы, закрыть используемые соединения
}

// Вывод в лог сообщений
logInfo "Старт сценария"

// Вызов требуемых для работы других сценариев, при условии, что ранее они никем не вызывались 
// (например описание соединений и таблиц) 
callScripts пакет.ИмяСкрипта1, пакет.ИмяСкрипта2

// Описание соединения Oracle и установка его по умолчанию для Oracle объектов
useOracleConnection oracleConnection('ora', true) {
	connectHost = 'host'
	connectDatabase = 'db'
	login = 'user'
	password = 'password'
}

// Описание таблицы Oracle
oracleTable('oracle:table', true) {
	// Нахождение таблицы в БД
    schemaName = 'demo'; tableName = 'table1'
    // Поля таблицы
    field('id') { type = integerFieldType; isKey = true }
    field('name') { length = 50; isNull = false }
    field('dt') { type = datetimeType; isNull = false }
	
	// Создание таблицы
	create()
	
	// Запись 1000 записей в таблицу
	etl.rowsTo {
		writeRow { add ->
			(1..1000).each { num ->
				add id: num, name: "Row $num", dt: new Date()
			}
		}
	}
}

// Описание соединения Vertica
verticaConnection('ver', true) {
    // Считать параметры соединения с загруженной конфигурации
    useConfig 'конфигурация1'

    // Логировать все команды к Vertica, если сценарий вызван из класса тестирования
    ifUnitTestMode {
        sqlHistoryFile = 'путь/vertica.{date}.sql'
    }
	
    // Вызывать при подключении соединения к БД
    whenConnecting {
        executeCommand 'SET SEATCH_PATH TO demo, public;'
    }
}

// Описание таблицы Vertica
verticaTable('vertica:table', true) {
    // Соединение для таблицы
    useConnection verticaConnection('ver')
    // Нахождение таблицы в БД
    schemaName = 'demo'; tableName = 'table1'
    // Поля таблицы
    field('id') { type = integerFieldType; isKey = true }
    field('name') { length = 50; isNull = false }
    field('dt') { type = datetimeType; isNull = false }
    // Свойства создания таблицы
    createOpts {
        partitionBy = 'Year(dt) * 100 + Month(dt)'
        orderBy = ['dt', 'name']
        segmentedBy = 'Hash(id) ALL NODES'
    }
	dropOpt { ifExists = true }
	readOpts { where = 'id > 0' }
	writeOpts { batchSize = 10000L }
	bulkOpts { isGzip = true }
    // Методы таблицы
    create(ifNotExists: true)
	assert exists && countRows() == 0
}

// Профилирование выполнения блоков кода сценария
profile('Копирование из A в Б') {
    // Копирование с источника в приёмник через etl оператор
    etl.copyRows(oracleTable('oracle:table'), verticaTable('vertica:table'))
    // Вызвать сценарий, передав ему параметром проверяемую таблицу
    callScript пакет.Проверка, { параметр1 = verticaTable('vertica:table') }
    // Выполнение SQL скриптов на источнике 
    sql(verticaConnection('ver')) {
        vars.переменная1=значение // можно использовать в SQL скриптах
        runFile 'resource://script1.sql'
        if (vars.переменная2 == 'INVALID') // переменную можно установить в SQL скрипте и получить значение
			// Выполнения SQL скрипта из текста
            exec '''
                    IF (SELECT Count(*) FROM demo.table1) > 100;
                        FOR SELECT id AS category_id FROM demo.categories WHERE name = {переменная2};
                            DELETE FROM demo.table1 WHERE group_id = {category_id};   
                        END FOR;
                        COMMIT;
                    END IF;
                    '''
    }   
}

// Описание модели эталонных данных
models.referenceVerticaTables('ref_model', true) {
	// Соединение Vertica
	useReferenceConnection 'ver'
	// Схема хранения эталонных данных в БД
	referenceSchemaName = '_ref_schema'
	
	// Сохранять эталонные данные для таблицы
	referenceFromTable('vertica:table') { allowCopy = true }
	
	// Развернуть схему и таблицы для хранения эталонных данных
	createReferenceTables()
	
	// Сохранить данные из исходной таблицы в эталонные данные
	copyFromSourceTables()
}

// Включить многопоточный режим работы
thread {
	// Установить список обрабатываемых данных в потоках
	useList [1,2,3,4,5]
	// Задать количество одновременных потоков
	countProc = 3
	
	// Код выполнения в потоке
	run { num ->
		println "Вызван элемент $num}
	}
}

// Выполнение тестовых проверок
testCase {
    assertEquals(oracleTable('tableA').countRow(), verticaTable('tableB').countRow())
}
```

# Работа с источниками
Getl поддерживает работу с множеством источников данных:
* JDBC источники: DB2, Firebird, GreenPlum, H2, Hive, Impala, MS SQLServer, MySQL, Netezza, NetSuite, Oracle, PostgreSQL, Vertica;
* Файловые форматы: CSV, Excel, Json, XML, Yaml;
* Облачные системы: SalesForce.

## Копирование записей с источника в приёмник
В Getl реализован интеллектуальный менеджер копирования данных между источниками, в котором поддерживается:
* Автоматическая связь полей источника и приёмника по их именам с возможностью отключения функции;
* Автоматическое приведение типов значений копируемых полей записей из источника в приёмник с возможностью отключения функции;
* Указание связи копируемых полей вручную;
* Проверка, преобразование и обработка копируемых записей в собственном коде;
* Поддержка отложенного копирования в временные CSV файлы с дальнейшей пакетной загрузкой полученных файлов в БД приёмника;
* Оптимизация скорости копирования наборов данных с помощью компиляции кода под конкретные структуры наборов данных в байт код для его выполнения на JVM.

Пример копирования всех записей между наборами данных с одинаковыми полями:
```groovy
// Объявление таблицы Oracle
def oratab = oracleTable {
    useConnection oracleConnection { // Объявление соединения Oracle для таблицы
        connectHost = 'oracle-host'
		connectDatabase = 'oradb'
        login = 'user'
		password = 'password'
    }
    schemaName = 'user'
	tableName = 'table1'
}

// Объявление таблицы Vertica
def vertab = verticaTable {
    useConnection verticaConnection { // Объявление соединения Vertica для таблицы
        connectHost = 'vertica-host1'
		connectDatabase = 'verdb'
        extended.backupservernode = 'vertica-host2,vertica-host3'
        login = 'user'
		password = 'password'
    }
    schemaName = 'stage'
	tableName = 'table1'
}

// Копирование всех записей из Oracle в Vertica
etl.copyRows(oratab, vertab)
```

Пример копирования всех таблиц из указанной схемы Oracle в таблицы с аналогичной структурой для схемы Vertica в многопоточном режиме. Требуется создать таблицы Vertica перед копированием, 
а копирование производить с помощью пакетной загрузки в таблицы Vertica:
```groovy
verticaConnection('ver', true) {
    connectHost = 'vertica-host1'
	connectDatabase = 'verdb'
    extended.backupservernode = 'vertica-host2,vertica-host3'
    login = 'user'
	password = 'password'
    schemaName = 'stage'
}

oracleConnection('ora', true) {
    connectHost = 'oracle-host'
	connectDatabase = 'oradb'
    login = 'user'
	password = 'password'
    
	// Получение списка таблиц из схемы "schema" и добавление их в репозиторий
    addTablesToRepository retrieveDatasets(schemaName: 'schema'), 'ora.schema'
	
	// Многопоточный режим работы
	thread {
		// Использовать для обработки потоков имена таблиц Oracle из репозитория
		useList listJdbcTables('ora.schema:*')
		// Выполнять обработку списка таблиц в четыре потока
		countProc = 4
	
		// Код обработки имени таблицы из списка в потоке
		run { tableName ->
			// Обрабатываемая таблица Oracle
			def oratab = oracleTable(tableName)
			
			// Описание таблицы Vertica
			def vertab = verticaTable {
				// Использовать соединение "ver"
				useConnection verticaConnection('ver')
				tableName = oratab.tableName
				// Адаптировать поля Oracle к типам Vertica
				setOracleFields oratab
				// Создать таблицу
				create()
			}
			// Копировать записи из Oracle в Vertica 
			etl.copyRows(oratab, vertab) {
				// Включить отложенное копирование через пакектную загрузку временных CSV файлов
				bulkLoad = true
				// Выгружать из Oracle таблицы в сжатые по GZ алгоритму CSV файлы
				bulkAsGZIP = true
			}
		}
    }
}
```
* Метод "_retrieveDatasets_" получит с указанной схемы БД список таблиц и зарегистрирует их в репозитории под заданным именем группы "_ora.schema_".
* Метод "_listJdbcTables_" вепрет список имен подходящих по заданной маске имени таблиц из репозитория.
* Метод "_setOracleFields_" позволяет назначить таблице Vertica поля из таблицы Oracle, конвертируя типы в подходящие и умножая на 2 текстовые поля, чтобы utf-8 могло вместить значения.
* Включение параметра "_bulkLoad_" в операторе "_copyRows_" приведет к тому, что данные из источника будут сначала выгружены в временный CSV файл и загружены в приёмник с помощью
пакетной загрузки файлов, если она поддерживается приёмником.

# Чтение и запись в источники данных
Getl умеет читать данные со всех источников и записывать в JDBC источники и CSV файлы. Для разных источников есть разные 
ограничения: например, нельзя вставить записи в Hive или Impala таблицы (ибо ну очень медленно), зато туда спокойно можно
загрузить записи с помощью поддерживаемой в Getl пакетной загрузки CSV файлов.

Продемонстрирую пример записи в таблицу Vertica на базе случайно генерируемых данных:
```groovy
def vercon = verticaConnetion {
    connectHost = 'vertica-host1'; connectDatabase = 'verdb'
    extended.backupservernode = 'vertica-host2,vertica-host3'
    login = 'user'; password = 'password'
    schemaName = 'stage'

    executeCommand 'CREATE SEQUENCE IF NOT EXISTS stage.s_sales INCREMENT BY 1000;'
}

def seq = sequence {
    useConnection vercon
    name = 'stage.s_sales'
    cache = 1000
}

verticaTable {
    useVerticaConnection vercon 

    tableName = 'random_data'
    field('id') { type = integerFieldType; isKey = true }
    field('name') { length = 50; isNull = false }
    field('dt') { type = datetimeType; isNull = false }
    field('value') { type = numericFieldType; length = 12; precision = 2 }
    if (!exists) create()

    etl.rowsTo {
        destParams.batchSize = 10000
        writeRow { add ->
            (1..10000).each {
                def row = [:]
                row.id = seq.nextval()
                row.name = GenerationUtils.GenerateString(50)
                row.dt = GenerationUtils.GenerateDateTime()
                row.value = GenerationUtils.GenerateNumeric(12, 2)

                add row
            }
        }
    }   
}
```
* В сценарии создается счетчик, который используется при заполнении таблицы данными. Таким образом, при каждом вызове
сценария, будет вставляться новая порция 10 тысяч случайно сгенерированных записей с уникальным ключом _id_.
* Размер кэша в счетчике указывает, с каким шагом выдаются номера в БД. Это позволяет снизить поток запросов на получение
нового номера счетчика, Getl при выдаче будет автоматически вести счетчик и при достижении номерного пула получать с БД
новое значение.
* Оператор _rowsTo_ вызываем блок записи в указанную таблицу. В данном примере он вызывается внутри кода таблицы, поэтому
ему не требуется указывать параметром, в какую таблицу будут записываться данные. 
* Вместо указания _batchSize_ в свойствах записи таблицы _writeOpts_, свойства можно указать для частного случае для
оператора записи с помощью _destParams_ (актуально и для copyRows).
* В _writeRow_ описывается код записи данных, в него параметром приходит объект записи в таблицу, при вызове которого
нужно передать запись.
* В Getl есть библиотека GenerationUtils, с помощью которой можно маскировать данные и генерировать случайные значения по
заданным правилам. Полезная штука для создания наборов данных для тестирования логики.
P.S. Как и для _copyRows_, в _rowsTo_ можно включить пакетную загрузку через промежуточный CSV файл опцией bulkLoad. 

Для чтения записей у таблиц есть функции _rows_ и _eachRow_. Запишем данные из полученной таблицы Vertica в Json и CSV файлы с применением
этих функций, выбрав по условию id меньше 1000:
```groovy
verticaTable { vertab ->
    useVerticaConnection vercon
    
    def csv = csv {
        fileName = '/data/files/file1.txt'
        fieldDelimiter = ';'
        codePage = 'utf-8' 
    }

    etl.rowsTo(csv) {
        writeRow { writer ->
            vertab.eachRow(where: 'id < 1000', order: ['id']) { row ->
                writer row
            }
        }
    }

    def writer = new File('/data/files/file1.json').newWriter('utf-8')
    def builder = new StreamingJsonBuilder(writer)
    builder(vertab.rows(where: 'id < 1000', order: ['id']))
    writer.close()
}
```
* При объявлении файловых источников в Getl можно не указывать соединение с помощью _useConnection_, если в имени файла 
_fileName_ прописано не только имя файла, но и путь.
* При указании параметров фильтрации _where, сортировки _order_ и количества возвращаемых записей _limit_, Getl транслирует
их в генерируемый для JDBC источников SELECT и использует возможности СУБД.
* Для записи в Json файл используется штатный билдер генерации Json, встроенный в Groovy. Аналогично есть билдеры для
генерации XML и Yaml файлов.

# Копирование файлов
Getl поддерживает работу со следующими файловыми системами: локальные, FTP, SFTP, HDFS. Можно копировать, перемещать, 
удалять и парсить группы файлов по заданным маскам пути и условиям.

Рассмотрим пример, где на FTP выкладываются CSV файлы по суточным директориям. Требуется копировать выявленные новые
файлы на локальную файловую систему и загружать их в таблицу Vertica:
```groovy
useVerticaConnection verticaConnetion('ver:con', true) {
    connectHost = 'vertica-host1'; connectDatabase = 'verdb'
    extended.backupservernode = 'vertica-host2,vertica-host3'
    login = 'user'; password = 'password'
}

verticaTable('data', true) {
    schemaName = 'stage'
    tableName = 'table1'
    assert exists, "В БД не найдена таблица $fullTableName!"
}

verticaTable('history', true) {
    schemaName = 'stage'
    tableName = 's_history_table1'
}

ftp('source', true) {
    server = 'ftp.domain'
    rootPath = '/files'
    login = 'user'; password = 'password'
    useStory verticaTable('history')
    createStory = true
}

files('dest', true) {
    rootPath = '/data/files/from_load'
}

fileman.fileCopier(ftp('source'), files('dest')) {
    useSourcePath {
        mask = '{date}/data_{region}.{num}.txt'
        variable('date') { type = dateFieldType; format = 'yyyyMMdd' }
        variable('num') { type = integerFieldType; length = 3 }
    }

    useDestinationpath {
        mask = '{region}/{date}'
    }

    numberAttempts = 3
    timeAttempts = 2
    order = ['date', 'num']
}

csv('bulk_file', true) {
    useConnection csvConnection { path = files('dest').rootPath }
    fieldDelimiter = '|'
    codePage = 'utf-8'
    field = verticaTable('data').field
}

verticaTable('data') {
    bulkLoadCsv(csv('bulk_file')) {
        files = filePath {
            mask = '{date}/data_{region}.{num}.txt'
            variable('date') { type = dateFieldType; format = 'yyyyMMdd' }
            variable('num') { type = integerFieldType; length = 3 }
        }
 
        loadAsPackage = true
        orderProcess = ['date', 'num']

        exceptionPath = csv('bulk_file').currentCsvConnection.path + '/vertica.bulkload.err'
        rejectedPath = csv('bulk_file').currentCsvConnection.path + '/vertica.bulkload.err'
        
        removeFile = true
    }
}
``` 
* В данном примере все объекты регистрируются в репозитории.
* Для того, чтобы каждый раз не задавать соединение при создании таблиц, используется оператор _useVerticaConnection_. 
При описании таблиц Vertica они будут автоматически работать на заданном соединении, если только для них явно не задано в
описании использовать другое соединение.
* В Vertica описывается таблица хранения данных и проверяется, что она есть в БД (свойство fullTableName вернет полное имя
таблицы в БД с учетом её схемы). Так же описывается таблица хранения истории скопированных файлов, она будет создана 
автоматически Getl при вызове копирования.
* Создается описание 2 файловых систем: источника на ftp с указанием таблицы хранения истории захвата данных и приёмника
на локальной файловой системе.
* _fileCopier_ отвечает за процесс копирования данных. При вызове передаются параметры источника и приёмника файловой 
системы (фс), задается маска захвата файлов, маска конечного расположения скопированных файлов в директориях от рутового 
пути приёмника, порядок копирования файлов и количество попыток при сбоях операций работы с фс.
* Для пакетной загрузки файлов в таблицы Vertica для них реализован метод _bulkLoadCsv_. При его вызове параметром указывается
CSV, который будет использован как шаблон загрузки (настройки, список полей, путь соединения). В _files_ можно задать
имя файла, список имен файлов, простую маску файлов или путь файлов с помощью _filePath_ объекта Getl (этот же тип используется
и при указании sourcePath и destinationPath в _fileCopier_). Опция _loadAsPackage_ указывает, что все найденные файлы
будут загружены одним вызовом COPY в Vertica с перечислением имен файлов. Если её отключить, то для каждого найденного
файла будет вызван свой COPY. Дополнительно указываются пути, в какие директории писать файлы ошибок для записей,
которые не прошли проверку Vertica на корректность. По окончании загрузки, исходные файлы будут удалены, 
так как включена опция _removeFile_.

В итоге, при вызове сценария, Getl просмотрит на ftp все директории, которые по имени могут быть распарсены в дату,
создаст на локальной фс нужную иерархию директорий, раскидает по ней файлы и одним запросом загрузит все файлы в 
Vertica. Несмотря на простоту, _fileCopier_ поддерживает множество режимов работы, в том числе зеркальное копирование 
на несколько источников одновременно или сегментное копирование на несколько источников по заданному хэш ключу. 
Последнее позволяет из одного источника взять файлы, раскидать их равномерно веером по кластеру ETL серверов и 
на каждом запустить свою обработку доставленных данных. Таким способом можно организовать распределенную обработку файлов.      

# Парсинг файлов
Организовать многопоточный парсинг файлов из источника не более сложно, чем их копировать. В примере нам потребуется
захватить из sftp источника json файлы по заданным условиям и записать их в Vertica. Усложним задачу тем, что в json
есть подчиненная структура "детализация", которую нужно сохранить в отдельную таблицу, то есть реализовать master-detail
схему: 
```groovy
useVerticaConnection verticaConnetion('ver:con', true) {
    connectHost = 'vertica-host1'; connectDatabase = 'verdb'
    extended.backupservernode = 'vertica-host2,vertica-host3'
    login = 'user'; password = 'password'
}

verticaTable('master', true) {
    schemaName = 'stage'
    tableName = 'master_table'
    assert exists, "В БД не найдена таблица $fullTableName!"
}

verticaTable('detail', true) {
    schemaName = 'stage'
    tableName = 'detail_table'
    assert exists, "В БД не найдена таблица $fullTableName!"
}

sftp('source', true) {
    server = 'ftp.domain'
    rootPath = '/files'
    login = 'user'; password = 'password'
    hostKey = 'ключ RSA'
}

json('source-file', true) {
    rootNode = 'data'
    field('id') { type = integerFieldType }
    field('dt') { type = datetimeFieldType; alias = 'datetime' }
    field('details') { type = objectFieldType }
}

fileman.fileProcessing(sftp('source')) {
    useSourcePath {
        mask = '{date}/data.{num}.json'
        variable('date') { type = dateFieldType; format = 'yyyyMMdd' }
        variable('num') { type = integerFieldType; length = 3 }
    }

    removeFiles = true
    removeEmptyDirs = true
    countOfThreadProcessing = 16
    threadGroupColumns = ['date']
    order = ['num']

    processFile { inf ->
        logFine "Парсинг файла ${inf.filepath}/${inf.filename} ..."
        json('source-file') {
            currentJSONConnection.path = inf.file.parent
            fileName = inf.file.name
        }

        etl.copyRows(json('source-file'), verticaTable('master')) {
            childs(verticaTable('detail')) {
                writeRow { add, source_row ->
                    source_row.details?.each { elem ->
                        add master_id: source_row.id, value: elem.value 
                    }
                }
            }
        }

        copyRow { source_row, dest_row ->
            assert source_row.id != null
        }

        inf.result = inf.completeResult
        logInfo "Из файла ${inf.filepath}/${inf.filename} загружено ${verticaTable('master').updateRows} " +
                "записей в master и ${verticaTable('detail')} записей в detail."
    }
}
```
* Для sftp требуется указать RSA ключ подключения в _hostKey_ или задав имя файла ключей в _knownHostsFile_. Для 
источника не указана таблица истории, если в ней появятся файлы с такими же именами, они снова будут загружены.
* В описании json файла указывается главная нода, с которой брать массив данных. Если в json данные сразу лежат в массиве
без именования ноды, то в _rootNode_ нужно поставить точку. По умолчанию для json, xml и yaml файлов Getl пытается
считывать значения полей из тех аналогичных имен нод в файлах, поэтому имена полей должны быть указаны с _field_
с учетом регистра. Если имя определяемого поля не совпадает с именем ноды в файле или же требуется задать путь получения 
поля (например поле находится в "группе.аттрибуте"), то точный способ получения нужно указать в _alias_. Так как поле
details в json файле является сложной структурой (массивом), то ему задается объектный тип. Парсинг таких полей возлагается
на код сценария.
* Для _fileProcessing_ указывается источник файлов и маска поиска. В примере задано удалять файлы и пустые директории, если
с них удалились файлы при захвате. В _countOfThreadProcessing_ задано парсить файлы в 16 одновременных потоков, группируя
файлы по датам, что задано в параметр _threadGroupColumns_. При парсинге задан порядок сортировки, однако он условный,
так как файлы будут обрабатываться в многопоточном режиме: гарантируется только, что на момент запуска парсера список для обработки
файлов будет отсортирован.
* Свой код для парсинга найденного файла в потоке указывается в _processFile_. На вход ему приходит дескриптор, в котором
определены атрибуты файла, есть ссылка на скаченный с источника в локальную временную директорию файл и ожидается, что по
окончании работы код в _result_ установит, с каким результатом завершилась обработка файлов.
* При процессинге json файлу устанавливается, по какому пути находится файл для обработки и как он называется. Как и говорилось
выше, этот json файл не является экземпляром описанного выше объекта в репозитории _source-file_, а является его клоном 
в потоке. Поэтому можно смело менять его атрибуты внутри потока, это никак не отразиться на объект в репозитории и на
работу других потоков.
* В _copyRows_ копируются совпадающие по именам поля из json файла в основную таблицу Vertica. Дополнительно в _childs_
задается, что так же требуется писать данные в детальную таблицу Vertica. На каждую запись источника Getl будет вызывать
код _writeRow_ дочернего приёмника. В код передается дескриптор записи в дочернюю таблицу и исходная запись источника.
Остается только перебрать массив, который лежит в json поле details и сохранить его значения в таблицу, подставляя еще
код мастер записи. Конструкция "source_row.details?.each" указывает Groovy, что нужно выполнить перебор записей только,
если поле details не имеет "null" значение. В Java нам пришлось бы писать условие "if (source_row.details != null)". 
* Если указан код _copyRow_, то он будет вызываться на каждую запись источника. В код будет приходить два параметра:
запись источника и подготовленные для записи данные приёмника. В данном случае мы ничего не меняем в значениях для
записи полей таблицы мастер, а проверяем, что нужные поля точно заполнены. В случае ошибки в assert, будет считаться,
что обработка файла завершена с ошибкой. 
* В зависимости от того, какое результат указан в _result_, Getl будет производить разный набор действий:
    * Для completeResult файл будет считаться успешно обработанным. Он будет перенесен в файловую системе хранения архивных файлов, 
    если она задана в _storageProcessedFiles_. Если включено _removeFiles_, то файл будет удален с источника.
    * для errorResult файл будет считаться ошибочным. Он будет перенесен в файловую систему хранения ошибочных файлов,
    если она задана в _storageErrorFiles_. Вместе с файлом рядом будет создан текстовый файл с описанием ошибки и 
    трассировкой его выполнения на момент ошибки. Удаляться на источнике файл не будет. Если включено _removeFiles_, 
    то файл будет удален с источника.
    * Для ignoreResult файл будет считаться игнорируемым. Он никуда не будет переноситься и не будет удаляться с источника.

При запуске сценария Getl с sftp получит список подходящих файлов, сгруппирует их по директориям и по очереди каждую обработает,
запуская код обработки файлов в 16 потоков. Для каждого найденного файла в Vertica будет создано отдельное соединение,
запущена транзакция, будут записаны данные в 2 таблицы и соединение закрыто. Все успешно обработанное будет удалено,
включая суточные директории на источнике.

# Выполнение пакетных сценариев SQL
Для построения консолидированного слоя и разработки аналитических витрин Getl предлагает собственный язык хранимых процедур (ХП).
В ХП встроена поддержка:
* Поочередное выполнение SQL операторов, разделенных точкой с запятой;
* Работа с переменными внутри скрипта;
* Установку значений переменных перед выполнением скрипта и получение их значений после выполнения;
* Получение в переменную количества обработанных записей после выполнения DML оператора SQL;
* Команду _SET_ для получения значений в переменные из полей записи;
* Команду _IF_ для выполнения блока команд по условию;
* Команду _FOR_ для выполнения блока команд в цикле;
* Команду _BEGIN BLOCK_ для выполнения блока команд в оригинальном виде без обработки языком ХП;
* Команду _ERROR_ для аварийного завершения работы SQL скрипта с указанной ошибкой;
* Команду _EXIT_ для выхода из скрипта с указанным кодом;
* Команду _ECHO_ для выдачи текста в консоль лога.

Для выполнения ХП скриптов можно использовать _exec_ в операторе _sql_:
```groovy
def vercon = verticaConnetion {
    connectHost = 'vertica-host1'; connectDatabase = 'verdb'
    extended.backupservernode = 'vertica-host2,vertica-host3'
    login = 'user'; password = 'password'
}

sql {
    // Использовать указанное соединение
    useConnection vercon

    // Установить значение переменным
    vars.param1 = 1
    vars.param2 = 'ok'
    vars.param3 = new Date()

    // Выполнить скрипт из текста
    exec '''
/*:count_row*/ 
UPDATE public.table1 
SET updatetime = '{param3}'::timestamp 
WHERE id = {param1} AND status = '{param2}'
    '''
    assert vars.count_row == 1, 'Не удалось обновить запись!'
}
```
* При выполнении ХП Getl вместо {переменных} подставляет их значения. Поэтому при использовании текстовых переменных 
внутри скриптов следует их обрамлять в одинарные кавычки.
* Для переменных с типом дата-время Getl подставляет текстовое значение в формате "yyyy-MM-dd HH:mm:ss".
* Если перед DML оператором поставить _/*:переменная*/_, то количество обработанных записей будет занесено в эту
переменную и она будет доступна в vars.

Так же большие скрипты можно вынести в отдельные файлы, в том числе ресурсные и вызывать их с помощью _runFile_ в
операторе sql. Создадим в ресурсах проекта файл /sql/script.sql:
```genericsql
ECHO Расчет региона {regionid} ...

-- Получение периода расчета
SET SELECT Min(hour) as hour_start, Max(hour) as hour_finish, Count(*) AS count_rows
    FROM public.table1 
    WHERE regionid = {regionid} AND status = 'incomplete';

-- Выйти, если ничего не выявлено
IF ({count_rows} = 0 OR '{hour_finish}'::timestamp < '2020-01-01'::timestamp);
  ECHO 'Записей для расчета не найдено!'
  EXIT;
END IF;

ECHO Выявлено {count_rows} записей с {hour_start} по {hour_finish}

-- Провести расчет в цикле по часам региона
FOR SELECT hour
    FROM public.table1
    WHERE regionid = {regionid} AND hour BETWEEN '{hour_start}'::timestamp AND '{hour_finish}'::timestamp;

    UPDATE public.table1
    SET status = 'complete'
    WHERE regionid = {regionid} AND hour = '{hour}'::timestamp

    ECHO Час {hour} успешно рассчитан.
END FOR; 
```
* При выполнении SET в переменные vars будут установлены значения из полей записи запроса, который указан в SELECT.
* Оператор _IF_ для проверки условия выполнит в БД запрос, поэтому в условии должно стоять выражение на языке SQL.
* Оператор _FOR_ выполнит в цикле операторы в своем теле, подставляя в переменные vars значения полей для каждой 
обрабатываемой записи. При обработке записей в цикле, оператор _FOR_ сначала получит в память весь массив записей и потом
начнем по нему обработку. Не рекомендуется возвращать в его _SELECT_ большое количество записей.

Вызов написанного sql файла:
```groovy
def vercon = verticaConnetion {
    connectHost = 'vertica-host1'; connectDatabase = 'verdb'
    extended.backupservernode = 'vertica-host2,vertica-host3'
    login = 'user'; password = 'password'
}

sql {
    // Использовать указанное соединение
    useConnection vercon

    // Установить значение переменным
    vars.regionid = 1

    // Выполнить скрипт из ресурсного файла
    runFile 'resource:/sql/script.sql'

    logInfo "Было обработано ${vars.count_rows} записей"
}
```  

# Расширения в Getl под Vertica
Бывают момент, когда требуется в витринах произвести перерасчет данных за период задним числом. Если это делать прямо
на рабочей витрине, то придется столкнуться с множеством проблем: удалить старые данные партицией нельзя,
иначе пользователи просто не увидят старых данных, пока не посчитаются новые и не сохраняться в транзакции. А удалять
с помощью оператора DELETE миллионы или миллиарды записей в Vertica приведет к большому времени выполнения такого оператора
и деградации работы SELECT с этой таблицей, пока не будет выполнен _purge_ после перерасчета витрины. Функция обмена партициями
идеально решает все эти проблемы, при условии, что периоды вписываются в таблице партициями:
```groovy
verticaTable('marts:dm1', true) {
    schemaName = 'marts'
    tableName = 'dm1'
}

verticaTable('#temp_dm1', true) {
    schemaName = 'stage'
    tableName = 'dm1'
    createLike verticaTable('marts:dm1'), true, true
    truncate()
}

// Код расчета периода витрины задним числом в стейджинговой области
/* ... */

verticaTable('marts:dm1') {
    swapPartitionsBetweenTables('начало периода', 'конец периода', verticaTable('#temp_dm1'))
    purgeTable()
    analyzeStatistics()
} 
```
* В сценарии мы описываем в репозитории временную таблицу в стейджинговой области, задав имени в репозитории решетку. 
После выполнения сценариев Getl автоматически удаляет все объекты, имена которых начинаются с решетки. Это позволяет 
создавать временные объекты в репозитории и гарантировать, что они перестанут существовать после финиша работы их владельца
и имена не пересекутся при повторном вызове этого сценария или в других сценариях. Функция createLike создаст полную копию
структуры таблицы из указанной витрины, включая правила партиционирования и весь список проекций у таблицы родителя. 
Таблица будет создана, если её ещё не существует. Функция _truncate_ гарантирует, что таблица будет пустой на момент выполнения.
* Вызов функции _swapPartitionsBetweenTables_ поменяет местами партиции между витриной и стейджинговой таблицей. После
её выполнения в витрине окажутся новые рассчитанные данные, а в стейджинговой таблице старые данные до расчета.
* После изменения большого объема данных будет не лишним у витрины Vertica вызывать дефрагментацию с помощью _purgeTable_
и обновить статистику по таблице с помощью _analyzeStatistics_.

У Vertica есть функции быстрого копирования данных между кластерами: _COPY FROM VERTICA_ и _EXPORT TO VERTICA_. Для облегчения
работы с этими функциями в SQL, Getl поддерживает подключение одного кластера Vertica к другому:
```groovy
verticaConnection('stand1', true) {
    connectHost = 'vertica-stand1'; connectDatabase = 'db1'
    login = 'user'; password = 'password'
}

verticaConnection('stand2', true) {
    connectHost = 'vertica-stand2'; connectDatabase = 'db2'
    login = 'user'; password = 'password'
}

verticaConnection('stand1') {
    attachExternalVertica verticaConnection('stand2')
    executeCommand 'COPY public.table1 FROM VERTICA public.table1' 
    detachExternalVertica verticaConnection('stand2')
}
``` 
* К соединению _stand1_ будет подключено внешнее соединение с _stand2_ и оттуда скопированы записи таблицы.
* Getl запоминает, какие внешние подключения были связаны с соединением Vertica. Не удастся второй раз присоединить тот же
кластер Vertica или разъединить не присоединенный ранее кластер.

Для оптимизации работы с Vertica полезно периодически проводить дефрагментацию таблиц, у которых высокий процент удаленных
записей, а также пересчитывать статистику. Getl позволяет упростить этот процесс и написать свой небольшой сценарий:
```groovy
verticaConnection {
    connectHost = 'vertica-host1'; connectDatabase = 'verdb'
    extended.backupservernode = 'vertica-host2,vertica-host3'
    login = 'user'; password = 'password'

    purgeTables(15)
    analyzeStatistics()
}
``` 
* _purgeTables_ сделает дефрагментацию всех таблиц БД Vertica, у которых процент удаленных записей от общего количества 
превышает 15%.
* _analyzeStatistics_ пересчитает статистику всех таблиц в БД Vertica.

В Vertica работает анализатор выполняемых запросов, задача которого идентифицировать проблемы, и записывать советы по их
решению в специальную служебную таблицу. Часть из них касается устаревания статистики по таблицам, где оптимизатор
из-за этого может начать использовать не самые оптимальные запросы работы с данными. Getl умеет находить такие рекомендации
и обновлять статистику у тех таблиц, о которых упомянул анализатор Vertica:
```groovy
verticaConnection {
    connectHost = 'vertica-host1'; connectDatabase = 'verdb'
    extended.backupservernode = 'vertica-host2,vertica-host3'
    login = 'user'; password = 'password'

    def problems = analyzeWorkload(new Date())
    processWorkload problems
}
```
* _analyzeWorkload_ запустит анализатор Vertica и попросит начиная с текущего времени проанализировать все проблемы по
таблицам профилирования запросов.
* _processWorkload_ пройдется по списку найденных проблем и пересчитает статистику для таблиц, которые указал анализатор.

# Работа со стендами
С помощью конфигураций в Getl можно упростить разработку, тестирование и работу проекта на разных стендах. В Getl
поддерживается GroovySlurper формат конфигураций. Фактически это Groovy DSL язык конфигураций, в котором Вы можете
указывать не только значения, но и Groovy выражения:
```groovy
раздел {
    массивы {
        a = [1,2,3]
        b = [4,5]
        c = a + b
    }
    d = new Date()
    e = 'Текущая дата'
    f = "$e: $d"
}
```
С помощью ключевого слова _environments_ в конфигурации можно разделить хранения информации по разным средам. Напишем
конфигурацию подключения к дев и прод стенду Vertica и сохраним как ресурсный файл _/data/vertica.connect.conf_:
```groovy
environments {
    dev {
        connections {
            con1 {
                connectHost = 'vertica-dev-host1'; connectDatabase = 'verdb'
                extended {
                    backupservernode = 'vertica-dev-host2,vertica-dev-host3'
                }
                login = 'user'; password = 'password'
            }   
        }
    }

    prod {
        connections {
            con1 {
                connectHost = 'vertica-prod-host1'; connectDatabase = 'verdb'
                extended {
                    backupservernode = 'vertica-prod-host2,vertica-prod-host3'
                }
                login = 'user'; password = 'password'
            }   
        }
    }
}
```
Напишем сценарий создания подключения к Vertica _data.ConnectToVertica_:
```groovy
package data

import groovy.transform.BaseScript
import getl.lang.Getl

@BaseScript Getl main

configuration {
    load 'resource:/data/vertica.connect.conf'
}

verticaConnection('ver:con', true) {
    useConfig 'con1'
}
```
* _useConfig_ указывает объекту Getl, что его свойства нужно взять из загруженной конфигурации. Для соединений параметры
нужно описывать в разделе _connections_ конфигураций, для файловых источников в разделе _files_.

Напишем ETL процесс работы со сценарием подключения
```groovy
package data

import groovy.transform.BaseScript
import getl.lang.Getl

@BaseScript Getl main

callScripts data.ConnectToVertica

verticaConnection('ver:con') {
    // Подключение будет к нужному стенду
}
```
* При вызове этого сценария из командной строки, он по умолчанию будет работать в _prod_ среде. При чтении файла конфигурации
будет читаться раздел environments.prod. 
* При вызове этого сценария из-под unit test класса в IDE, он по умолчанию будет работать в _dev_ среде и подключаться 
к стенду разработки.
* Можно явно указать при вызове сценария, в какой среде работать, с помощью параметра командной строки _environment=<среда>_.

Для процессов работы с JDBC источниками часто бывает необходимо использовать разные логины в ходе работы сценариев.
Для этого можно описать логины и пароли пользователей в раздельном разделе конфигурации. Создадим конфигурацию
в ресурсном файле _/data/vertica.logins.conf_:
```groovy
logins {
    vertica {
        user1 = 'password1'
        user2 = 'password2'            
    }
}
```
Подключим этот файл в сценарий описания соединения _data.ConnectToVertica_:
```groovy
package data

import groovy.transform.BaseScript
import getl.lang.Getl

@BaseScript Getl main

configuration {
    load 'resource:/data/vertica.connect.conf'
    load 'resource:/data/vertica.logins.conf'
}

verticaConnection('ver:con', true) {
    useConfig 'con1'
    storedLogins = configContent.logins.vertica
}
```
* При загрузке конфигураций Getl их объединяет в памяти. Если параметры нового файла перекрывают существующие, то 
они будут установлены поверх текущих значений.
* _storedLogins_ позволяет хранить для соединения разные логины и пароли пользователей.     

Теперь можно использовать переключение между пользователями Vertica в сценариях:
```groovy
package data

import groovy.transform.BaseScript
import getl.lang.Getl

@BaseScript Getl main

callScripts data.ConnectToVertica

verticaConnection('ver:con') {
    useLogin 'user1'
    // Код работы под логином user1 ...
    useLogin 'user2'
    // Код работы под логином user2 ...
}
```
* _useLogin_ переключит соединение на указанного пользователя, взяв его пароль из установленного списка пользователей 
для соединения.
* При переключении между логинами текущее соединение будет разорвано и создано под новым пользователем.
* Если заданного имени пользователя в определенном для соединения списке логинов нет, то будет сгенерирована ошибка. 

# Разработка кейсов тестирования
Для разработки юнит тестов в Getl есть специальный класс _GetlDslTest_. Создадим в тестовом модуле проекта класс 
тестирования сценария proc.Proc1, очистив перед его запуском все таблицы в схеме _demo_ и проверив, что в этих таблицах
появились записи после работы сценария:
```groovy
package proc

import getl.test.GetlDslTest
import getl.lang.Getl
import org.junit.Test

class Proc1Test extends GetlDslTest {
    @Test
    void testProc1() {
        Getl.Dsl {
            callScripts data.ConnectToVertica

            verticaConnection('ver:con').retrieveDatasets(schemaName: 'demo') { table ->
                table.truncate()
            }

            callScript proc.Proc1

            verticaConnection('ver:con').retrieveDatasets(schemaName: 'demo') { table ->
                assertTrue(table.countRows() > 0)
            }
        }
    }
} 
```
* _callScripts_ только один раз вызывает указанный скрипт. Если в Proc1 тоже вызывается этот скрипт через _callScripts_,
то он не будет вызван второй раз. Это гарантирует, что не будет второй попытки зарегистрировать соединение Vertica в 
репозитории.
* При запуске сценариев в _GetlDslTest_ классе, для них будет установлена среда выполнения dev. Так же внутри сценариев
сработает блок ifUnitTestMode, в котором можно описать логику работы сценария при запуске в таком режиме.
* При вызове разных @Test методов в классе, конфигурация и репозиторий Getl автоматически очищается. Для инициализации
Getl рекомендуется описать нужные действия в методе с аннотацией @Before.
* Если требуется запустить тест метод под другой средой выполнения, то ему нужно указать аннотацию @Config(env='<среда>'),
это позволяет сделать тесты для запуска на тестовой среде test или промышленной prod.

# Модели Getl
В большинстве случаев при разработке Etl задач приходится оперировать группами объектов. Например, скопировать данные из списка таблиц источника и загрузить 
в таблицы приёмника, переместить группу файлов на другой источник и т.д. В Getl поддерживаются различные модели группировки объектов, которые позволяют описать набор объектов и правила 
работы с ними.

## Модель referensFiles
Позволяет автоматизировать доставку и распаковку архивов эталонных файлов в указанное место. С помощью этой модели легко автоматизировать подготовку тестирования Etl процессов, которые
копируют или парсят файлы. В описании модели указываются два файловых менеджера, источник и приемник, а также перечисляются архивы, которые содержат эталонные файлы. 

Напишем класс ref.Model1, в котором объявим модель _model1_:
```groovy
package ref

import groovy.transform.BaseScript
import getl.lang.Getl

@BaseScript Getl main

ftp('source', true) {
	server = 'host'
	user = 'user'
	password = 'password'
	rootPath = '/reference_files'
}

files('dest', true) {
	rootPath = '/files_for_loading'
}

models.referensFiles('model1', true) {
	useSourceManager 'source'
	useDestinationManager 'dest'
	unpackCommand = '7z x -y -bd "{file}"'
	
	referenceFromFile('file1.7z') { destinationPath = 'dir1' }
	referenceFromFile('file2.7z') { destinationPath = 'dir2' }
}
```
* В качестве источника хранения эталонных файлов указан FTP сервер, файлы расположены в каталоге "reference_files". 
* В качестве места развертывания эталонных файлов указана локальная директория "files_for_loading". 
* Для модели задана команда распаковки архивов с помощью архиватора 7z, который должен быть установлен и виден в пути ОС.
* Для модели задано два эталонных архива, они должны присутствовать на FTP сервере, иначе модель выдаст ошибку.
* Для каждого архива задана своя локальная директория, куда будут развернуты после доставки. После распаковки сами архивы будут удалены с локального директория.
* В команде распаковки 7z задана опция развертывания файлов с сохранением структуры, поэтому в архивах можно сохранить директории и файлы, нужные для тестирования
процессов.

После описания модели, ее можно использовать в юнит-тестах и разворачивать нужные файлы перед запуском тестируемого Etl процесса:
```groovy
package proc

import getl.test.GetlDslTest
import getl.lang.Getl
import org.junit.Test
import ref.Model1

class Proc2Test extends GetlDslTest {
    @Test
    void testProc2() {
        Getl.Dsl {
            callScripts Model1
            models.referensFiles('model1') { fill() }
            callScript proc.Proc2
			// Код проверки ...
        }
    }
} 
```
1. Вызывается вышеописанный скрипт описания модели.
1. Модель доставляет архивы и разворачивает их файлы в нужное место с помощью метода _fill_.
1. Запускается тестируемый процесс, который при запуске гарантировано будет работать с одним набором эталонных файлов.
1. В конце юнит-теста можно описать логику проверки работы тестируемого процесса.

## Модель referenceVerticaTables
Позволяет автоматизировать хранение и копирование эталонных данных для таблиц Vertica. С помощью этой модели легко автоматизировать подготовку тестирования Etl процессов, которые
работают с таблицами Vertica. В описании модели указывается соединение к Vertica, имя схемы БД, в которой будут хранится эталонные данные и перечисляются таблицы Vertica, 
для которых нужно хранить эталонные данные.

Напишем класс ref.Model2, в котором объявим модель _model2_ и заполним ее эталонными данными:
```groovy
package ref

import groovy.transform.BaseScript
import getl.lang.Getl

@BaseScript Getl main

useVerticaConnection verticaConnection('ver', true) {
	connectHost = 'host1'
	connectDatabase = 'db'
	login = 'user'
	password = 'password'
}

verticaTable('ver:dim1', true) {
	schemaName = 'public'
	tableName = 'dim1'
}

verticaTable('ver:dim2', true) {
	schemaName = 'public'
	tableName = 'dim2'
}

verticaTable('ver:fact', true) {
	schemaName = 'public'
	tableName = 'fact'
}

verticaTable('ver:mart', true) {
	schemaName = 'public'
	tableName = 'mart'
}

models.referenceVerticaTables('model2', true) {
	useReferenceConnection 'ver'
	referenceSchemaName = '_ref_model2'
	
	modelVars.point = DateUtils.ParseDate('2020-01-01')
	
	referenceFromTable('ver:dim1') { allowCopy = true }
	referenceFromTable('ver:dim2') { whereCopy = "'{point}'::date BETWEEN start_date AND finish_date" }
	referenceFromTable('ver:fact') { 
		whereCopy = "fact_date <= '{point}'::date"
		sampleCopy = 10
	}
	referenceFromTable('ver:mart')
}
```
* Для модели задано соединение Vertica, указываемые в модели таблицы должны быть описаны под этим соединением.
* _ref_model2_ указывает имя схемы в БД Vertica, где будут развернуты таблицы для хранения эталонных данных.
* В переменной модели_point_ задана дата, на которую будут формироваться эталонные данные
* В модели описано 4 таблицы, для которых требуется хранить эталонные данные:
	* Таблица _ver:dim1_: флаг _allowCopy_ указывает, что нужно сохранить все записи этого справочника в эталонные данные
	* Таблица _ver:dim2_: фильтр _whereCopy_ определяет, что в эталонные данные нужно скопировать только те записи этого справочника, у которых период действия попадает 
	под указанную в _point_ дату.
	* Таблица _ver:fact_: фильтр _whereCopy_ определяет, что в эталонные данные нужно скопировать записи таблицы фактов по указанную в _point_ дату, а _sampleCopy_ указывает, что 
	нужно взять 10% сэмпл от исходных данных для хранения в качестве эталона.
	* Таблица _ver:mart_: так как параметры заполнения были не заданы, то эталонная таблица будет пустой.

После определения модели требуется создать схему и таблицы эталонных данных:
```groovy
models.referenceVerticaTables('model2') { createReferenceTables() }
```
* В БД Vertica будет создана схема _ref_model2_ и в ней таблицы, которые будут полностью соответствовать структуре исходных таблиц, включая проекции и партиции. Пользователь, 
под которым было подключение к Vertica должен иметь соответствующие разрешения на создание схем в БД и доступ на чтение и запись к исходным таблицам, указанным в модели. 
* Если таблицы уже присутствуют в БД, то они будут пропущены и не пересозданы. Для пересоздания таблиц при вызове метода _createReferenceTables_ требуется указать в параметре _recreate_
значение _true_.

Теперь можно сохранить эталонные данные из исходных таблиц:
```groovy
models.referenceVerticaTables('model2') { copyFromSourceTables() }
```
* Записи из исходных таблиц будут скопированы в таблицы хранения эталонных данных с использованием указанных условий с помощью оператора "INSERT SELECT" и включенным директом или 
с помощью функции копирования партиций, если таблица имеет партиционирование.
* Если в эталонной таблице уже ранее были сохранены записи, то в нее не будут копироваться записи. Для перезаписи эталонных данных при вызове метода _copyFromSourceTables_ требуется
указать в параметре _onlyForEmpty_ значение _false_.

Так же эталонные данные можно получить из таблиц другого кластера Vertica, это позволяет заполнить тестовый стенд реальными данными из рабочего кластера Vertica:
```groovy
def prod = verticaConnection { 
	connectHost = 'prod-host1'
	connectDatabase = 'db'
	user = 'user'
	password = 'password'
}

models.referenceVerticaTables('model2') { copyFromVertica(prod) }
```
* Пользователь, подключенный к внешнему кластеру Vertica должен иметь разрешения на чтений таблиц, с которых будут читаться эталонные данные.
* Внешний кластер Vertica должен видеть используемый кластер Vertica, где хранятся эталонные данные.
* При захвате эталонных данных будет создано соединение между кластерами Vertica и произведено копирование данных командой "EXPORT TO VERTICA".
* Если в эталонной таблице уже ранее были сохранены записи, то в нее не будут копироваться записи. Для перезаписи эталонных данных при вызове метода copyFromVertica требуется
указать вторым параметром _onlyForEmpty_ значение _false_.

После описания и заполнения модели, ее можно использовать в юнит-тестах и разворачивать нужные данные в таблицах перед запуском тестируемого Etl процесса:
```groovy
package proc

import getl.test.GetlDslTest
import getl.lang.Getl
import org.junit.Test
import ref.Model2

class Proc3Test extends GetlDslTest {
    @Test
    void testProc2() {
        Getl.Dsl {
            callScripts Model2
            models.referensVerticaTables('model2') { fill() }
            callScript proc.Proc3
			// Код проверки ...
        }
    }
} 
```
1. Вызывается скрипт описания модели "_model2_".
1. Модель копирует записи из эталонных таблиц в исходные таблицы Vertica с помощью метода "_fill_". Если в исходных таблицах находится такое же количество записей, 
то они не заполняются заново. 
1. Запускается тестируемый процесс, который при запуске гарантировано будет работать с одним набором эталонных данных.
1. В конце юнит-теста можно описать логику проверки работы тестируемого процесса.

## Модель setOfTables
Позволяет описать группу таблиц, которые можно использовать в процессах или шаблонах, если требуется обработать более одной таблицы. В описании модели указывается соединение и 
перечисляются таблицы этого соединения.

Напишем класс "_sets.Model3_", в котором объявим модель "_model3_":
```groovy
package sets

import groovy.transform.BaseScript
import getl.lang.Getl
import getl.utils.DateUtils

@BaseScript Getl main

useVerticaConnection verticaConnection('ver', true) {
	connectHost = 'host1'
	connectDatabase = 'db'
	login = 'user'
	password = 'password'
}

verticaTable('ver:table1', true) {
	schemaName = 'public'
	tableName = 'table1'
}

verticaTable('ver:table2', true) {
	schemaName = 'public'
	tableName = 'table2'
}

models.setOfTables('model3', true) {
	useSourceConnection 'ver'
	
	modelVars.delete_date = DateUtils.CurrentDate()

	table('ver:table1') {
		attrs.method = 'TRUNCATE'
	}
	
	table('ver:table2') {
		attrs.method = 'DELETE'
		attrs.where = "DT <= '{delete_date}'::date"
	}
}
```
1. Для модели указывается Vertica соединение "_ver_".
1. В модели описывается переменная _delete_date_, которая используется для условия удаления записей в "_table2_".
1. В модель добавляется две таблицы из Vertica.
1. Для каждой таблицы назначаются атрибуты, которые можно использовать в процессах и шаблонах.

Напишем шаблон _patterns.CleanTables_, задача которого очищать список таблиц по заданным правилам:
```groovy
package patterns

import groovy.transform.BaseScript
import getl.lang.Getl
import groovy.transform.Field
import getl.models.SetOfTables
impoty static getl.utils.StringUtils.WithGroupSeparator

@BaseScript Getl main

@Field SetOfTables tables

void check() {
	assert tables != null, 'Список таблиц не указан!'
	tables.checkAttrs ['method', 'where']
}

logFine "Очистка ${tables.usedTables.size()} таблиц ..."
tables.usedTables.each { node ->
	def table = node.sourceTable
	def method = (node.attrs.method as String)?.toUpperCase()
	def where = node.attrs.where as String
	
	assert method in ['TRUNCATE', 'DELETE'], "Неизвестный метод очистки \"$method\""
	if (method == 'TRUNCATE) {
		table.truncate()
		logInfo "Таблица \"$table\" успешно очищена"
	}
	else {
		def count = table.deleteRows(where)
		logInfo "В таблице \"$table\" успешно удалено ${WithGroupSeparator(count)} записей"
	}
}
```
1. В шаблоне разрешено использовать два атрибута параметров таблиц: "_method_" и "_where_". С помощью метода "_checkAttrs_" у модели вызывается проверка атрибутов таблиц на эти значения.
Если заданы неизвестные атрибуты, будет ошибка.
1. Для каждой таблицы модели в зависимости от заданного метода вызывается полная очистка таблицы "_truncate_" или удаление записей "_deleteRows_". В атрибуте "_where_" можно задать условие
удаление записей.

Теперь можно использовать шаблон в unit-тесте:
```groovy
package patterns

import getl.test.GetlDslTest
import getl.lang.Getl
import org.junit.Test
import sets.Model3

class CleanTablesTest extends GetlDslTest {
    @Test
    void testCleanTables() {
        Getl.Dsl {
            callScripts Model3
            callScript CleanTables, [tables: models.setOfTables('model3')]
			models.setOfTables('model3').usedTables.each { node ->
				assertEquals(0, node.sourceTable.countRow())
			}
        }
    }
} 
```
1. Вызывается скрипт описания модели "_model3_".
1. Вызывается шаблон очистки таблиц, которому передается набор таблиц из модели "_model3_".
1. Для каждой таблицы модели "_model3_" проверяется, что в ней нет записей.

## Модель mapTables
Позволяет описать маппинг таблиц между источником и приёмником. В описании модели указываются соединения источника и приёмника и связи между таблицами этих источников.

Напишем класс "_maps.Model4_", в котором объявим модель "_model4_":
```groovy
package maps

import groovy.transform.BaseScript
import getl.lang.Getl

@BaseScript Getl main

useOracleConnection oracleConnection('ora', true) {
	connectHost = 'host1'
	connectDatabase = 'db'
	login = 'user'
	password = 'password'
}

oracleTable('ora:table1', true) {
	schemaName = 'user'
	tableName = 'table1'
}

oracleTable('ora:table2', true) {
	schemaName = 'user'
	tableName = 'table2'
}

useVerticaConnection verticaConnection('ver', true) {
	connectHost = 'host2'
	connectDatabase = 'db'
	login = 'user'
	password = 'password'
}

verticaTable('ver:table1', true) {
	schemaName = 'public'
	tableName = 'table1'
}

verticaTable('ver:table2', true) {
	schemaName = 'public'
	tableName = 'table2'
	writeOpts { batchSize = 10000 }
}

models.mapTables('model4', true) {
	useSourceConnection 'ora'
	useDestinationConnection 'ver'
	
	mapTables('ora:table1') {
		linkTo 'ver:table1'
		attrs.hints = 'PARALLEL (10)'
		attrs.where = 'field1 IS NOT NULL'
	}
	
	mapTables('ora:table2') {
		linkTo 'ver:table2'
		map.fact_date = 'dt'
		listPartitions = ['2020-01-01', '2020-01-02', '2020-01-03']
	}
}
```
1. Для модели указывается соединения к источнику Oracle и приёмнику Vertica.
1. Таблица Oracle "_table1_" связывается с таблицей Vertica "_table1_". В атрибутах задается хинт для чтения и условие выборки для таблицы Oracle.
1. Таблица Oracle "_table2_" связывается с таблицей Vertica "_table2_". В маппинг полей задается, что поле "_dt_" из Oracle таблицы будет записываться в поле "_fact_date_" таблицы Vertica.
Перечисляется список партиций из 3 суток, которые нужно скопировать из исходной в приёмную таблицу.

Напишем шаблон "_CopyOracleToVertica_", задача которого копировать данные из таблиц Oracle в таблицы Vertica по заданным правилам:
```groovy
package patterns

import groovy.transform.BaseScript
import getl.lang.Getl
import groovy.transform.Field
import getl.models.MapTables
impoty static getl.utils.StringUtils.WithGroupSeparator

@BaseScript Getl main

@Field MapTables maps
@Field Integer countThreads = 1

void check() {
	assert maps != null && !maps.isEmpty(), 'Маппинг таблиц не указан!'
	assert countThreads?:0 > 0, 'Количество потоков должно быть больше нуля!'
	tables.checkAttrs ['hints', 'where']
}

logFine "Копирование ${tables.usedTables.size()} таблиц из Oracle в Vertica ..."
thread {
	useList usedMapping
	setCountProc countThreads
	run { MapTableSpec node ->
		def source = node.source as OracleTable
		def dest = node.destination as VerticaTable
		def maps = node.map
		
		source.readOpts { 
			if (node.attrs.hints != null)
				hints = node.attrs.hints 
			if (node.attrs.where != null)
				where = node.attrs.where
		}
		
		def isPartition = !node.listPartitions.isEmpty()
		def listPartition = (isPartition)?node.listPartitions:[0]
		listPartition.each { part ->
			if (isPartition)
				source.readOpts { usePartition = part }
				
			etl.copyRows(source, dest) { maps = maps }
			logInfo "Из таблицы \"$source\" прочитано ${WithGroupSeparator(source.readRows)} записей, в таблицу \"$dest\" записано ${WithGroupSeparator(dest.updateRows)} записей"
		}
	}
}
logInfo "Копирование завершено."
```
1. В шаблоне разрешено использовать два атрибута параметров таблиц: "_hints_" и "_where_". С помощью метода "_checkAttrs_" у модели вызывается проверка атрибутов таблиц на эти значения.
Если заданы неизвестные атрибуты, будет ошибка.
1. Для каждой таблицы маппинга в шаблоне создается свой поток выполнения кода.
1. Для таблицы источника Oracle из атрибутов модели устанавливаются хинты и условие выборки.
1. Из правил маппинга таблицы определяется, есть ли список партиций, по которым нужно скопировать записи.
1. Для каждой партиции в цикле выполняется копирование записей. Для таблицы "_table1_" партциии не указаны, поэтому будет скопирована вся таблиица. Для таблицы "_table2_" список
был задан в модели, поэтому они будут скопированы с использованием хинта Oracle для выборки всех записей партиции. При копировании указывается маппинг, заданный в модели. 
Для таблицы "_table2_" помимо автоматического маппинга по именам полей таблиц источника и приёмника, будут копироваться значения из поля "_dt_" Oracle таблицы в поле "_fact_date_" Vertica
таблицы.

Теперь можно использовать шаблон в unit-тесте:
```groovy
package patterns

import getl.test.GetlDslTest
import getl.lang.Getl
import org.junit.Test
import sets.Model4

class CopyOracleToVerticaTest extends GetlDslTest {
    @Test
    void testCopyOracleToVertica() {
        Getl.Dsl {
            callScripts Model4
            callScript CopyOracleToVertica, [tables: models.mapTables('model4'), countThreads = 2]
			models.mapTables('model4').usingMapping.each { node ->
				assertEquals(node.source.countRow(), node.destination.countRow())
			}
        }
    }
} 
```
1. Вызывается скрипт описания модели "_model4_".
1. Вызывается шаблон копирования таблиц, которому передается маппинг таблиц из модели "_model4_" с указанием копирования двух таблиц одновременно.
1. Для каждого правила маппинга модели "_model4_" проверяется, что после копирования количество записей в исходной и приёмной таблице совпадает.